{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting the variables from .env file and setting up as a environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "#Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model =\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give input and get response\n",
    "prompt_op = llm.invoke(\"What is generative API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command takes time because langchain api key records all events that are happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A generative API is an application programming interface designed to facilitate the creation of content using generative models. These models are typically based on artificial intelligence and machine learning techniques, particularly neural networks, that can produce new data instances similar to existing data. Here's a brief overview of what a generative API might involve:\\n\\n1. **Generative Models**: These are algorithms that can create new content based on patterns learned from training data. Examples include Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models like GPT (Generative Pre-trained Transformer).\\n\\n2. **Content Creation**: Generative APIs can produce various types of content, such as text, images, music, code, or even more complex data structures. For example, OpenAI's GPT-3 API can generate human-like text based on prompts provided by users.\\n\\n3. **Use Cases**: Generative APIs can be used in a wide range of applications including content creation for blogs and articles, automated code generation, image synthesis for design and art, creating virtual characters or environments in gaming, and developing chatbots or virtual assistants.\\n\\n4. **Access and Integration**: Developers can integrate generative APIs into their applications to leverage AI-driven content generation capabilities. These APIs typically provide endpoints that accept input data and return generated content, often with options to customize the output based on parameters.\\n\\n5. **Ethical and Practical Considerations**: While generative APIs offer powerful capabilities, they also raise ethical concerns, such as the potential for misuse in generating misleading or harmful content. Additionally, considerations around bias, copyright, and content moderation are important in their deployment.\\n\\nOverall, generative APIs are a key component in the growing field of AI-driven content creation, providing powerful tools for developers and businesses to automate and enhance their creative processes.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_op.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prompt template\n",
    "how you want your llm to behave or what kind of role you want to give your LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### chat prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answers based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above prompt template along with LLM Model using chain. this will tell teh LLM Model how it need to behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_response = chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangSmith is a platform designed to enhance the development and deployment of applications that use large language models (LLMs). It offers tools for debugging, testing, evaluating, and monitoring LLM-based applications, making it easier for developers to build more reliable and efficient AI systems. LangSmith provides features such as prompt management, version control, and detailed analytics, allowing developers to optimize the performance of their language model applications effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 83, 'prompt_tokens': 33, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_159d8341cc', 'finish_reason': 'stop', 'logprobs': None}, id='run-e19e42fd-1322-4cb3-b2d0-2628dd3f35fa-0', usage_metadata={'input_tokens': 33, 'output_tokens': 83, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using output parser to display messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "OpParser_Chain = prompt|llm|parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpParser_op_response = OpParser_Chain.invoke({\"input\":\"Can you tell me about LangSmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a platform developed by LangChain, designed to enhance the development, testing, and evaluation of applications built with large language models (LLMs). It offers a comprehensive suite of tools to assist developers in various stages of application development:\\n\\n1. **Monitoring and Debugging**: LangSmith provides features to monitor the performance of LLM applications in real-time, allowing developers to identify and debug issues effectively.\\n\\n2. **Evaluation**: The platform facilitates the evaluation of LLM applications, offering insights into how well the models are performing and where improvements can be made.\\n\\n3. **Experimentation**: Developers can use LangSmith to run experiments, testing different model configurations or approaches to determine which works best for their specific use case.\\n\\n4. **Deployment**: It simplifies the deployment process, enabling developers to move from development to production seamlessly.\\n\\nLangSmith integrates deeply with LangChainâ€™s framework, leveraging its capabilities to build powerful and efficient LLM applications. This makes it particularly useful for developers working within the LangChain ecosystem, providing them with the necessary tools to optimize and refine their language model-based applications.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OpParser_op_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
